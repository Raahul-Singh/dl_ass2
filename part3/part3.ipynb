{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, transform=None):\n",
    "        self.training_sketches = sorted(glob('dataset/training_i/cropped_sketches/*.jpg'))\n",
    "        self.training_photos = sorted(glob('dataset/training_ii/cropped_photos/*.jpg'))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.training_sketches)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        X = Image.open(self.training_sketches[index]).resize((256, 256))\n",
    "        Y = Image.open(self.training_photos[index]).resize((256, 256))\n",
    "\n",
    "        if self.transform:\n",
    "            X = self.transform(X)\n",
    "            Y = self.transform(Y)\n",
    "\n",
    "        return (X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.ToTensor()\n",
    "training_dataset = ImageDataset(transform=transform)\n",
    "training_dataloader = DataLoader(training_dataset, batch_size=4, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(kernel, in_channels, out_channels, dropout=False):\n",
    "    \"\"\"Downsample or Upsample based on kernel and in/out dimensions.\"\"\"\n",
    "    \n",
    "    layers = []\n",
    "    layers.append(kernel(in_channels, out_channels, kernel_size=4, stride=2, padding=1))\n",
    "    layers.append(nn.BatchNorm2d(num_features=out_channels))\n",
    "    if dropout:\n",
    "        layers.append(nn.Dropout())\n",
    "    layers.append(nn.LeakyReLU())\n",
    "\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.generator = nn.Sequential(\n",
    "\n",
    "            # Downsample\n",
    "            sample(nn.Conv2d, 1, 64),                              # [64, 128, 128]\n",
    "            sample(nn.Conv2d, 64, 128),                            # [128, 64, 64]\n",
    "            sample(nn.Conv2d, 128, 256),                           # [256, 32, 32]\n",
    "            sample(nn.Conv2d, 256, 256, dropout=True),             # [256, 16, 16]\n",
    "            sample(nn.Conv2d, 256, 256, dropout=True),             # [256, 8, 8]\n",
    "\n",
    "            # Upsample\n",
    "            sample(nn.ConvTranspose2d, 256, 256, dropout=True),    # [256, 16, 16]\n",
    "            sample(nn.ConvTranspose2d, 256, 256),                  # [256, 32, 32]\n",
    "            sample(nn.ConvTranspose2d, 256, 128),                  # [128, 64, 64]\n",
    "            sample(nn.ConvTranspose2d, 128, 64),                   # [64, 128, 128]\n",
    "            sample(nn.ConvTranspose2d, 64, 3),                     # [3, 256, 256]\n",
    "\n",
    "        )\n",
    "\n",
    "    def forward(self, input_image):\n",
    "        # input_image.shape = [1, 256, 256]\n",
    "        return self.generator(input_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.discriminator = nn.Sequential(\n",
    "            sample(nn.Conv2d, 6, 64),                              # [64, 128, 128]\n",
    "            sample(nn.Conv2d, 64, 128),                            # [128, 64, 64]\n",
    "            sample(nn.Conv2d, 128, 256),                           # [256, 32, 32]\n",
    "            nn.Conv2d(256, 128, kernel_size=4, padding=1),         # [128, 31, 31]\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(128, 64, kernel_size=4, padding=1),          # [64, 30, 30]\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(64, 1, kernel_size=4, padding=1)             # [1, 29, 29] \n",
    "        )\n",
    "\n",
    "    def forward(self, input_image, target_image):\n",
    "        # input_image.shape = [3, 256, 256]\n",
    "        # target_image.shape = [3, 256, 256]\n",
    "        concat = torch.cat([input_image, target_image], dim=1)\n",
    "        return self.discriminator(concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan_loss_1 = nn.BCEWithLogitsLoss()\n",
    "gan_loss_2 = nn.MSELoss()\n",
    "\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "optimizer_G = torch.optim.Adam(generator.parameters())\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('our_stuff': conda)",
   "name": "python388jvsc74a57bd0ad78d6be4a5732df2d539e8d326d5b0092d3bf7959edddaefa5331d15236f690"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "metadata": {
   "interpreter": {
    "hash": "ad78d6be4a5732df2d539e8d326d5b0092d3bf7959edddaefa5331d15236f690"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}